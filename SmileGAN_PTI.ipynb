{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SmileGAN-PTI: Counterfactual Image Generation for Visual Attribute Editing\n",
        "\n",
        "## Author: Mohammad Mosaffa (mm3322@cornell.edu)"
      ],
      "metadata": {
        "id": "m3_QKnci-tw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx3ZILAx-pr9"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "!pip install ninja imageio-ffmpeg ipywidgets matplotlib torchvision gdown\n",
        "!pip install face-alignment\n",
        "!pip install scipy scikit-image\n",
        "!pip install lpips\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install clip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup: Clone e4e Repository & Download Pre‑trained Models\n",
        "\n",
        "This section prepares the environment by cloning the **`encoder4editing` (e4e)** repository and downloading four pre‑trained assets that work together during inversion, fine‑tuning, and expression editing.\n",
        "\n",
        "### Core Models\n",
        "\n",
        "1. **`e4e_ffhq_encode.pt` (e4e Encoder)**\n",
        "   * **Type:** Encoder  \n",
        "   * **Purpose:** Converts a real input face image into a StyleGAN‑friendly latent code.  \n",
        "   * **Key Feature:** Latent codes are deliberately positioned to remain *highly editable* while still preserving identity.  \n",
        "   * **Dataset:** Trained on FFHQ (high‑quality human faces).\n",
        "\n",
        "2. **`stylegan2‑ffhq‑config‑f.pt` (StyleGAN2 Generator)**\n",
        "   * **Type:** Generator (Image Synthesizer)  \n",
        "   * **Purpose:** Takes a latent code—original or edited—and synthesises a 1024 × 1024 face.  \n",
        "   * **Key Feature:** Industry‑standard visual fidelity and detail.  \n",
        "   * **Dataset:** Trained on FFHQ, matching the encoder.\n",
        "\n",
        "### Add‑on Components\n",
        "\n",
        "3. **`smile.pt` (Expression Direction)**\n",
        "   * **Type:** Latent‑space edit vector (W⁺)  \n",
        "   * **Purpose:** When added to the latent code, it biases mid‑level layers toward a *smiling* expression.  \n",
        "   * **Key Feature:** Learned with InterfaceGAN; neutral enough to leave pose, hair and lighting unaffected.  \n",
        "   * **Usage:** Scaled by a factor `α` (e.g., 0.7) and injected only into expression‑controlling layers (8–11).\n",
        "\n",
        "4. **`model_ir_se50.pth` (ArcFace IR‑SE‑50)**\n",
        "   * **Type:** Face‑recognition network  \n",
        "   * **Purpose:** Provides an *identity loss* during PTI so the generator fine‑tunes without drifting from the person’s real‑world likeness.  \n",
        "   * **Key Feature:** Lightweight but strong at capturing identity embeddings; widely used in PTI/e4e pipelines.  \n",
        "   * **Dataset:** Trained on MS‑1M and refined for FFHQ compatibility.\n",
        "\n",
        "---\n",
        "\n",
        "### How They Work Together\n",
        "\n",
        "1. **Encode** the photograph with the *e4e Encoder* → latent code *W⁺*.  \n",
        "2. **Fine‑tune** the *StyleGAN2 Generator* via PTI, using *ArcFace IR‑SE‑50* to keep identity intact.  \n",
        "3. **Edit** the code by adding `α × smile.pt` in expression layers.  \n",
        "4. **Synthesize** the final 1024‑px smiling portrait with the tuned generator.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ Model‑Download Issues?\n",
        "\n",
        "If automatic downloads fail:\n",
        "\n",
        "| Asset | Direct Link | Target Path |\n",
        "|-------|-------------|-------------|\n",
        "| e4e Encoder | `https://drive.google.com/uc?id=1h2uBzmFhbuox2g9-vKBC9iTcxS5IS1pk` | `e4e/pretrained_models/e4e_ffhq_encode.pt` |\n",
        "| StyleGAN2 Generator | `https://drive.google.com/uc?id=1V4rGkG8H7QtO_uVfrvy3aZhHFi3fY4UZ` | `e4e/pretrained_models/stylegan2-ffhq-config-f.pt` |\n",
        "| Smile Direction | `https://raw.githubusercontent.com/yuval-alaluf/latent-diffusion-directions/main/edit_directions/smile.pt` | `e4e/pretrained_models/smile.pt` |\n",
        "| ArcFace IR‑SE‑50 | `https://github.com/TreB1eN/InsightFace_Pytorch/raw/master/model_ir_se50.pth` | `e4e/pretrained_models/model_ir_se50.pth` |\n",
        "\n",
        "Make sure all four files reside in **`e4e/pretrained_models/`** before running the pipeline.\n"
      ],
      "metadata": {
        "id": "4UQJGkYuBk7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/omertov/encoder4editing.git e4e\n",
        "%cd e4e\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "os.makedirs(\"pretrained_models\", exist_ok=True)\n",
        "\n",
        "# Download e4e encoder checkpoint\n",
        "e4e_url = \"https://drive.google.com/uc?id=1h2uBzmFhbuox2g9-vKBC9iTcxS5IS1pk\"\n",
        "e4e_output = \"pretrained_models/e4e_ffhq_encode.pt\"\n",
        "gdown.download(e4e_url, e4e_output, quiet=False)\n",
        "\n",
        "# Download StyleGAN2 FFHQ generator\n",
        "stylegan_url = \"https://drive.google.com/uc?id=1V4rGkG8H7QtO_uVfrvy3aZhHFi3fY4UZ\"\n",
        "stylegan_output = \"pretrained_models/stylegan2-ffhq-config-f.pt\"\n",
        "gdown.download(stylegan_url, stylegan_output, quiet=False)\n",
        "\n",
        "# smile latent direction\n",
        "!git clone https://github.com/yuval-alaluf/latent-diffusion-directions.git latent_dirs\n",
        "\n",
        "shutil.copy(\n",
        "    \"latent_dirs/edit_directions/smile.pt\",\n",
        "    \"pretrained_models/smile.pt\",\n",
        ")\n",
        "print(\"smile.pt copied to pretrained_models/\")\n",
        "\n",
        "# ArcFace IR‑SE‑50 (ID loss net)\n",
        "!git clone https://github.com/TreB1eN/InsightFace_Pytorch.git arcface_tmp\n",
        "shutil.copy(\n",
        "    \"arcface_tmp/model_ir_se50.pth\",\n",
        "    \"pretrained_models/model_ir_se50.pth\",\n",
        ")\n",
        "print(\"model_ir_se50.pth copied to pretrained_models/\")\n",
        "\n",
        "\n",
        "%cd /content/e4e"
      ],
      "metadata": {
        "id": "WE6iIqrBBZch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Block 1 — Environment Setup & Imports\n",
        "\n",
        "This cell loads the core libraries (PyTorch, LPIPS, face_alignment, NumPy, PIL, etc.), and tries to spin up the face‑landmark detector—warning you if the package is missing.\n",
        "\n",
        "It also wipes any remnants of previous runs and creates a clean folder structure: input_images/ for your source photos, output_smiles/ for the edited results, and optimized_data/ for latents and intermediate checkpoints. Absolute paths to all required weight files are defined here, along with quick knobs (SMILE_ALPHA, layer range) for controlling the smile edit later on."
      ],
      "metadata": {
        "id": "zh3ugT1VBw5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage import transform as trans\n",
        "import matplotlib.pyplot as plt\n",
        "import lpips\n",
        "import glob\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    import face_alignment\n",
        "except ImportError:\n",
        "    face_alignment = None\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if face_alignment:\n",
        "    try:\n",
        "        fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False, device=device)\n",
        "    except Exception:\n",
        "        fa = None\n",
        "else:\n",
        "    fa = None\n",
        "\n",
        "BASE_PROJECT_PATH = \"/content/\"\n",
        "E4E_BASE_PATH = os.path.join(BASE_PROJECT_PATH, \"e4e\")\n",
        "\n",
        "INPUT_FOLDER_PATH = os.path.join(BASE_PROJECT_PATH, \"input_images\")\n",
        "OUTPUT_FOLDER_PATH = os.path.join(BASE_PROJECT_PATH, \"output_smiles\")\n",
        "OPTIMIZED_DATA_FOLDER = os.path.join(OUTPUT_FOLDER_PATH, \"optimized_data\")\n",
        "\n",
        "print(f\"Please ensure your input images are placed in: {INPUT_FOLDER_PATH}\")\n",
        "\n",
        "if os.path.exists(OUTPUT_FOLDER_PATH):\n",
        "    try:\n",
        "        shutil.rmtree(OUTPUT_FOLDER_PATH)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "os.makedirs(INPUT_FOLDER_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
        "os.makedirs(OPTIMIZED_DATA_FOLDER, exist_ok=True)\n",
        "\n",
        "e4e_checkpoint_path = os.path.join(E4E_BASE_PATH, 'pretrained_models/e4e_ffhq_encode.pt')\n",
        "smile_direction_path = os.path.join(E4E_BASE_PATH, 'pretrained_models/smile.pt')\n",
        "id_loss_checkpoint_path_absolute = os.path.join(E4E_BASE_PATH, \"pretrained_models/model_ir_se50.pth\")\n",
        "\n",
        "processed_image_pairs = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FLDlbrEBoFc",
        "outputId": "ae966894-37be-43cc-ab1f-23d1b5b37a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please ensure your input images are placed in: /content/input_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 2 — Face Alignment Function\n",
        "\n",
        "Block 2 defines a function that detects facial landmarks in an input image, aligns the face to a standardized 256×256 format, and optionally saves a landmark verification plot for visual checking. It returns both the aligned face image and the landmark coordinates, preparing the data for downstream editing steps."
      ],
      "metadata": {
        "id": "G1okWZy7Ex0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_face(image_path, fa_model, output_path_prefix=\"\"):\n",
        "    if fa_model is None:\n",
        "        raise RuntimeError(\"Face alignment model (fa) is not initialized. Cannot proceed with alignment.\")\n",
        "\n",
        "    print(f\"Aligning face for image: {image_path}\")\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Input image not found at {image_path}\")\n",
        "\n",
        "    img_array = np.array(image)\n",
        "    landmarks_list = fa_model.get_landmarks(img_array)\n",
        "\n",
        "    if landmarks_list is None or len(landmarks_list) == 0:\n",
        "        raise ValueError(f\"No face detected in the image: {image_path}\")\n",
        "\n",
        "    landmarks = landmarks_list[0]\n",
        "\n",
        "    verification_image_path = f\"{output_path_prefix}_landmarks_verification.png\"\n",
        "    if output_path_prefix:\n",
        "        verification_dir = os.path.dirname(verification_image_path)\n",
        "        if verification_dir:\n",
        "            os.makedirs(verification_dir, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(img_array)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], c='red', s=10, marker='.')\n",
        "    plt.title(f\"Landmarks: {os.path.basename(image_path)}\")\n",
        "    plt.axis('off')\n",
        "    try:\n",
        "        plt.savefig(verification_image_path)\n",
        "        print(f\"Landmark verification image saved to: {verification_image_path}\")\n",
        "    except Exception as e_save:\n",
        "        print(f\"Warning: Could not save landmark verification image to {verification_image_path}. Error: {e_save}\")\n",
        "    plt.close()\n",
        "\n",
        "    left_eye_center = landmarks[36:42].mean(axis=0)\n",
        "    right_eye_center = landmarks[42:48].mean(axis=0)\n",
        "    nose_tip = landmarks[30]\n",
        "\n",
        "    target_left_eye = (96, 112)\n",
        "    target_right_eye = (160, 112)\n",
        "    target_nose_tip = (128, 160)\n",
        "\n",
        "    src_pts = np.array([left_eye_center, right_eye_center, nose_tip], dtype=np.float32)\n",
        "    dst_pts = np.array([target_left_eye, target_right_eye, target_nose_tip], dtype=np.float32)\n",
        "\n",
        "    tform = trans.SimilarityTransform()\n",
        "    tform.estimate(src_pts, dst_pts)\n",
        "    aligned_array = trans.warp(img_array, tform.inverse, output_shape=(256, 256), mode='reflect', preserve_range=True)\n",
        "\n",
        "    if aligned_array.dtype in [np.float64, np.float32]:\n",
        "        aligned_array = (np.clip(aligned_array, 0, 1) * 255).astype(np.uint8) if aligned_array.max() <= 1 else aligned_array.astype(np.uint8)\n",
        "    else:\n",
        "        aligned_array = aligned_array.astype(np.uint8)\n",
        "\n",
        "    aligned_pil_image = Image.fromarray(aligned_array)\n",
        "    return aligned_pil_image, landmarks\n"
      ],
      "metadata": {
        "id": "NF6_saH0E4ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 3 — Image Preprocessing and Mouth Mask Creation\n",
        "\n",
        "Block 3 defines the preprocessing transformations used for different model inputs, such as the e4e inversion model, PTI targets, and identity loss networks. It also provides a function that creates a mouth mask by detecting the mouth region from facial landmarks on the aligned image. This mask can be used later to guide localized edits or loss calculations focused on the mouth area."
      ],
      "metadata": {
        "id": "wCr7chk4F_kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_transform_for_inversion = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "preprocess_transform_for_pti_targets = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "preprocess_transform_1024_for_pti_targets = transforms.Compose([\n",
        "    transforms.Resize((1024, 1024)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "preprocess_transform_for_id_loss = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def create_mouth_mask_from_aligned_image(aligned_pil_image, fa_model, device_to_use):\n",
        "    if fa_model is None:\n",
        "        print(\"Face alignment model not available for mouth mask. Using a zero mask.\")\n",
        "        return torch.zeros((1, 1, 256, 256), device=device_to_use)\n",
        "\n",
        "    aligned_img_np = np.array(aligned_pil_image)\n",
        "    landmarks_on_aligned_list = fa_model.get_landmarks(aligned_img_np)\n",
        "\n",
        "    if not landmarks_on_aligned_list:\n",
        "        print(\"Could not detect landmarks on the aligned image for mouth mask. Using a zero mask.\")\n",
        "        return torch.zeros((1, 1, 256, 256), device=device_to_use)\n",
        "\n",
        "    landmarks_on_aligned = landmarks_on_aligned_list[0]\n",
        "    mouth_lm_indices = list(range(48, 68))\n",
        "    mouth_landmarks_points = landmarks_on_aligned[mouth_lm_indices]\n",
        "\n",
        "    min_x = int(np.min(mouth_landmarks_points[:, 0]))\n",
        "    max_x = int(np.max(mouth_landmarks_points[:, 0]))\n",
        "    min_y = int(np.min(mouth_landmarks_points[:, 1]))\n",
        "    max_y = int(np.max(mouth_landmarks_points[:, 1]))\n",
        "\n",
        "    current_mouth_mask = torch.zeros((1, 1, 256, 256), device=device_to_use)\n",
        "    min_y_clamped, max_y_clamped = max(0, min_y), min(255, max_y)\n",
        "    min_x_clamped, max_x_clamped = max(0, min_x), min(255, max_x)\n",
        "\n",
        "    if min_y_clamped < max_y_clamped and min_x_clamped < max_x_clamped:\n",
        "        current_mouth_mask[:, :, min_y_clamped:max_y_clamped, min_x_clamped:max_x_clamped] = 1.0\n",
        "        print(\"Mouth mask created for current image.\")\n",
        "    else:\n",
        "        print(\"Warning: Mouth landmark coordinates invalid for mask. Using zero mask.\")\n",
        "\n",
        "    return current_mouth_mask\n"
      ],
      "metadata": {
        "id": "YO7xHkRFGAM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 4 — Global Model Loading (e4e, LPIPS, Smile Direction, ID Loss)\n",
        "\n",
        "Block 4 handles the critical technical step of globally loading all machine learning models and components needed across the entire pipeline. It first sets up the e4e encoder by importing the pSp model class and safely loading the pretrained checkpoint (using both weights_only=True and fallback methods if needed). It also loads the smile direction vector, which is a latent-space edit that will later be applied to create smiling expressions.\n",
        "\n",
        "Next, it prepares the perceptual loss function (LPIPS) using the VGG backbone, ensuring it is ready for evaluating visual similarity during fine-tuning. Finally, the block attempts to load and initialize the identity loss model (ArcFace IR-SE-50), which plays a crucial role during PTI optimization by ensuring that edits preserve the subject’s identity. Careful handling of the current working directory (CWD) is included to ensure the IDLoss class finds its associated weight files."
      ],
      "metadata": {
        "id": "KxdWpnBgG0zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Namespace:\n",
        "    def __init__(self, dictionary):\n",
        "        self.__dict__.update(dictionary)\n",
        "\n",
        "try:\n",
        "    from models.psp import pSp\n",
        "    from criteria.id_loss import IDLoss\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"Import Error: {e}. Trying to add e4e path...\")\n",
        "    e4e_repo_path_check = os.path.join(BASE_PROJECT_PATH, \"e4e\")\n",
        "    if os.path.isdir(e4e_repo_path_check) and e4e_repo_path_check not in sys.path:\n",
        "        sys.path.append(e4e_repo_path_check)\n",
        "        print(f\"Added {e4e_repo_path_check} to sys.path for model imports.\")\n",
        "        from models.psp import pSp\n",
        "        from criteria.id_loss import IDLoss\n",
        "    else:\n",
        "        print(\"CRITICAL: Could not find pSp or IDLoss model class. Ensure 'e4e' directory is available.\")\n",
        "        raise\n",
        "\n",
        "if not os.path.exists(e4e_checkpoint_path):\n",
        "    raise FileNotFoundError(f\"CRITICAL: e4e checkpoint not found at {e4e_checkpoint_path}.\")\n",
        "\n",
        "try:\n",
        "    base_ckpt = torch.load(e4e_checkpoint_path, map_location='cpu', weights_only=True)\n",
        "    print(\"Base e4e checkpoint loaded (weights_only=True).\")\n",
        "except Exception as e_load_base:\n",
        "    print(f\"Warning: Could not load base e4e checkpoint with weights_only=True ({e_load_base}). Attempting without...\")\n",
        "    base_ckpt = torch.load(e4e_checkpoint_path, map_location='cpu')\n",
        "    print(\"Base e4e checkpoint loaded (full pickle).\")\n",
        "\n",
        "base_e4e_opts = Namespace(base_ckpt['opts'])\n",
        "base_e4e_opts.checkpoint_path = e4e_checkpoint_path\n",
        "base_e4e_opts.device = device\n",
        "\n",
        "if not os.path.exists(smile_direction_path):\n",
        "    raise FileNotFoundError(f\"CRITICAL: Smile direction file not found at {smile_direction_path}\")\n",
        "smile_direction_global = torch.load(smile_direction_path, map_location=device)\n",
        "print(f\"Global smile direction loaded with shape: {smile_direction_global.shape}\")\n",
        "\n",
        "perceptual_loss_fn_global = lpips.LPIPS(net='vgg').to(device).eval()\n",
        "print(\"Global LPIPS model loaded.\")\n",
        "\n",
        "id_loss_fn_global = None\n",
        "if not os.path.exists(id_loss_checkpoint_path_absolute):\n",
        "    print(f\"WARNING: Identity Loss checkpoint not found at {id_loss_checkpoint_path_absolute}. PTI will run without ID Loss.\")\n",
        "else:\n",
        "    original_cwd = os.getcwd()\n",
        "    if os.path.isdir(E4E_BASE_PATH):\n",
        "        print(f\"Temporarily changing CWD to {E4E_BASE_PATH} for IDLoss initialization.\")\n",
        "        os.chdir(E4E_BASE_PATH)\n",
        "    else:\n",
        "        print(f\"Warning: E4E_BASE_PATH ({E4E_BASE_PATH}) not found. IDLoss might fail if it uses relative paths.\")\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to instantiate IDLoss()...\")\n",
        "        id_loss_fn_global = IDLoss()\n",
        "        id_loss_fn_global.to(device)\n",
        "        id_loss_fn_global.eval()\n",
        "        print(f\"Global Identity Loss model instantiated and moved to device. CWD during init: {os.getcwd()}\")\n",
        "    except Exception as e_id_loss:\n",
        "        print(f\"Error initializing or loading Identity Loss model: {e_id_loss}\")\n",
        "        print(\"   Ensure 'model_ir_se50.pth' is in 'e4e/pretrained_models/'. PTI will run without ID Loss.\")\n",
        "        id_loss_fn_global = None\n",
        "    finally:\n",
        "        if os.getcwd() != original_cwd:\n",
        "            os.chdir(original_cwd)\n",
        "            print(f\"Restored CWD to: {original_cwd}\")"
      ],
      "metadata": {
        "id": "b9e4dM7bG3Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 — Alignment, Inversion, and PTI (Optimization Loop)\n",
        "\n",
        "Block 5 is the technical heart of the pipeline, where the system processes each input image through three major stages: face alignment, latent inversion, and PTI (Pivotal Tuning Inversion) optimization.\n",
        "\n",
        "First, it loops through all images in the input folder, detects and aligns each face, and saves the aligned result. Next, it performs inversion using the e4e encoder, which generates an initial latent code representing the face inside the StyleGAN latent space. This latent code is then passed into Stage 0: latent optimization, where the code itself is fine-tuned (without changing the generator) using a combination of pixel-level (L2) and perceptual (LPIPS) losses.\n",
        "\n",
        "After that, Stage 1 begins: generator fine-tuning, where the generator’s weights are adjusted while keeping the optimized latent fixed. This phase uses a multi-scale loss design—combining L1 and LPIPS at 256×256 and 1024×1024 resolutions, optionally mouth-specific L1 losses (if a mask was generated), and identity preservation penalties (via ArcFace ID loss if available). Throughout, the block saves intermediate outputs (like optimized latents and tuned generator weights) and logs progress."
      ],
      "metadata": {
        "id": "X-Lf5nCCHMWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimization_results = []\n",
        "\n",
        "image_extensions = ('*.jpg', '*.jpeg', '*.png')\n",
        "image_files = []\n",
        "for ext in image_extensions:\n",
        "    image_files.extend(glob.glob(os.path.join(INPUT_FOLDER_PATH, ext)))\n",
        "\n",
        "if not image_files:\n",
        "    print(f\"No images found in {INPUT_FOLDER_PATH}. Please add images and try again.\")\n",
        "else:\n",
        "    print(f\"Found {len(image_files)} images for optimization.\")\n",
        "\n",
        "for current_input_image_path in image_files:\n",
        "    current_image_filename = os.path.basename(current_input_image_path)\n",
        "    current_filename_no_ext = os.path.splitext(current_image_filename)[0]\n",
        "    print(f\"\\n--- Optimizing: {current_image_filename} ---\")\n",
        "\n",
        "    current_output_prefix_optimized = os.path.join(OPTIMIZED_DATA_FOLDER, current_filename_no_ext)\n",
        "    optimized_latent_path = f\"{current_output_prefix_optimized}_optimized_latent.pt\"\n",
        "    optimized_generator_path = f\"{current_output_prefix_optimized}_optimized_generator.pt\"\n",
        "    aligned_image_for_opt_path = f\"{current_output_prefix_optimized}_aligned_for_opt.jpg\"\n",
        "\n",
        "    try:\n",
        "        align_verif_prefix = os.path.join(OUTPUT_FOLDER_PATH, current_filename_no_ext)\n",
        "        aligned_image_pil, original_landmarks = align_face(current_input_image_path, fa, align_verif_prefix)\n",
        "        aligned_image_pil.save(aligned_image_for_opt_path)\n",
        "        print(f\"Aligned image for optimization saved to: {aligned_image_for_opt_path}\")\n",
        "\n",
        "        input_tensor_for_inversion = preprocess_transform_for_inversion(aligned_image_pil).unsqueeze(0).to(device)\n",
        "        mouth_mask_tensor = create_mouth_mask_from_aligned_image(aligned_image_pil, fa, device)\n",
        "\n",
        "        inversion_e4e_model = pSp(base_e4e_opts)\n",
        "        inversion_e4e_model.eval()\n",
        "        inversion_e4e_model.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, initial_latent_code = inversion_e4e_model(input_tensor_for_inversion, randomize_noise=False, return_latents=True)\n",
        "\n",
        "        original_generator = inversion_e4e_model.decoder\n",
        "        original_generator.eval()\n",
        "\n",
        "        print(\"Starting PTI Stage 0: Latent Optimization...\")\n",
        "        optimized_latent_code = initial_latent_code.clone().detach().requires_grad_(True)\n",
        "        optimizer_latent = optim.AdamW([optimized_latent_code], lr=0.01)\n",
        "        num_steps_latent_opt = 10\n",
        "\n",
        "        original_tensor_256_for_pti_0_1 = preprocess_transform_for_pti_targets(aligned_image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        for step in range(num_steps_latent_opt):\n",
        "            optimizer_latent.zero_grad()\n",
        "            generated_output_latent_opt = original_generator([optimized_latent_code], input_is_latent=True, randomize_noise=False)\n",
        "            generated_images_1024_raw_latent_opt = generated_output_latent_opt[0] if isinstance(generated_output_latent_opt, tuple) else generated_output_latent_opt\n",
        "            generated_1024_latent_opt_0_1 = (generated_images_1024_raw_latent_opt.clamp(-1, 1) + 1) / 2.0\n",
        "            generated_256_latent_opt_0_1 = F.interpolate(generated_1024_latent_opt_0_1, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "            l2_loss_latent_opt = F.mse_loss(generated_256_latent_opt_0_1, original_tensor_256_for_pti_0_1)\n",
        "            lpips_input_gen_latent_opt = generated_256_latent_opt_0_1 * 2 - 1\n",
        "            lpips_input_target_latent_opt = original_tensor_256_for_pti_0_1 * 2 - 1\n",
        "            lpips_loss_latent_opt = perceptual_loss_fn_global(lpips_input_gen_latent_opt, lpips_input_target_latent_opt).mean()\n",
        "\n",
        "            total_loss_latent_opt = l2_loss_latent_opt + lpips_loss_latent_opt\n",
        "            total_loss_latent_opt.backward()\n",
        "            optimizer_latent.step()\n",
        "\n",
        "            if step % 100 == 0 or step == num_steps_latent_opt - 1:\n",
        "                print(f\"Latent Opt Step {step}/{num_steps_latent_opt} - Loss: {total_loss_latent_opt.item():.4f}\")\n",
        "\n",
        "        print(\"Latent Optimization completed.\")\n",
        "        current_latent_code = optimized_latent_code.detach()\n",
        "\n",
        "        del original_generator, inversion_e4e_model\n",
        "        torch.cuda.empty_cache() if device == 'cuda' else None\n",
        "\n",
        "        print(\"Starting PTI Stage 1: Generator Tuning...\")\n",
        "        pti_model_instance = pSp(base_e4e_opts)\n",
        "        pti_generator = pti_model_instance.decoder\n",
        "        pti_generator.train()\n",
        "        pti_generator.to(device)\n",
        "\n",
        "        original_tensor_1024_for_pti_0_1 = preprocess_transform_1024_for_pti_targets(aligned_image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        if id_loss_fn_global:\n",
        "            target_image_for_id = preprocess_transform_for_id_loss(aligned_image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        optimizer_pti = optim.AdamW(pti_generator.parameters(), lr=0.002, weight_decay=0.02)\n",
        "        num_steps_pti_val = 1800\n",
        "\n",
        "        prev_loss = float('inf')\n",
        "        no_improvement = 0\n",
        "        patience = 20\n",
        "        min_delta = 1e-4\n",
        "\n",
        "        print(f\"Starting PTI Generator Tuning (targets in [0,1], lr={optimizer_pti.defaults['lr']}, steps={num_steps_pti_val})...\")\n",
        "        for step in range(num_steps_pti_val):\n",
        "            optimizer_pti.zero_grad()\n",
        "            generated_output_pti_loop = pti_generator([current_latent_code], input_is_latent=True, randomize_noise=False)\n",
        "            generated_images_1024_raw_loop = generated_output_pti_loop[0] if isinstance(generated_output_pti_loop, tuple) else generated_output_pti_loop\n",
        "            generated_1024_for_loss_0_1 = (generated_images_1024_raw_loop.clamp(-1, 1) + 1) / 2.0\n",
        "            generated_256_for_loss_0_1 = F.interpolate(generated_1024_for_loss_0_1, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "            l1_256 = F.l1_loss(generated_256_for_loss_0_1, original_tensor_256_for_pti_0_1)\n",
        "            l1_1024 = F.l1_loss(generated_1024_for_loss_0_1, original_tensor_1024_for_pti_0_1)\n",
        "\n",
        "            lpips_input_gen_256 = generated_256_for_loss_0_1 * 2 - 1\n",
        "            lpips_input_target_256 = original_tensor_256_for_pti_0_1 * 2 - 1\n",
        "            lpips_input_gen_1024 = generated_1024_for_loss_0_1 * 2 - 1\n",
        "            lpips_input_target_1024 = original_tensor_1024_for_pti_0_1 * 2 - 1\n",
        "            lpips_256 = perceptual_loss_fn_global(lpips_input_gen_256, lpips_input_target_256).mean()\n",
        "            lpips_1024 = perceptual_loss_fn_global(lpips_input_gen_1024, lpips_input_target_1024).mean()\n",
        "\n",
        "            l1_mouth_val = torch.tensor(0.0).to(device)\n",
        "            if mouth_mask_tensor.sum() > 0:\n",
        "                l1_mouth_val = F.l1_loss(generated_256_for_loss_0_1 * mouth_mask_tensor, original_tensor_256_for_pti_0_1 * mouth_mask_tensor)\n",
        "\n",
        "            id_loss_val = torch.tensor(0.0).to(device)\n",
        "            if id_loss_fn_global:\n",
        "                generated_for_id = F.interpolate(generated_images_1024_raw_loop, size=(112, 112), mode='bilinear', align_corners=False)\n",
        "                id_loss_output = id_loss_fn_global(generated_for_id, target_image_for_id, target_image_for_id)\n",
        "                id_loss_val = id_loss_output[0].mean() if isinstance(id_loss_output, tuple) else id_loss_output.mean()\n",
        "\n",
        "            w_l1 = 1.0\n",
        "            w_lpips = 1.3\n",
        "            w_l1_mouth = 2.0\n",
        "            id_loss_weight = 0.1\n",
        "\n",
        "            total_loss_pti_loop = (w_l1 * l1_256 + w_lpips * lpips_256) + \\\n",
        "                                  0.5 * (w_l1 * l1_1024 + w_lpips * lpips_1024) + \\\n",
        "                                  w_l1_mouth * l1_mouth_val + \\\n",
        "                                  id_loss_weight * id_loss_val\n",
        "\n",
        "            total_loss_pti_loop.backward()\n",
        "            optimizer_pti.step()\n",
        "\n",
        "            if step % 100 == 0 or step == num_steps_pti_val - 1:\n",
        "                id_loss_print = f\" | ID: {id_loss_val.item():.4f}\" if id_loss_fn_global and isinstance(id_loss_val, torch.Tensor) and id_loss_val.numel() > 0 else \"\"\n",
        "                print(f\"PTI Gen Step {step}/{num_steps_pti_val} - Loss: {total_loss_pti_loop.item():.4f}{id_loss_print}\")\n",
        "\n",
        "            if total_loss_pti_loop.item() >= prev_loss - min_delta:\n",
        "                no_improvement += 1\n",
        "                if no_improvement >= patience:\n",
        "                    print(f\"Early stopping at step {step} due to no improvement.\")\n",
        "                    break\n",
        "            else:\n",
        "                no_improvement = 0\n",
        "            prev_loss = total_loss_pti_loop.item()\n",
        "\n",
        "        print(\"PTI Generator Tuning completed.\")\n",
        "\n",
        "        torch.save(current_latent_code, optimized_latent_path)\n",
        "        torch.save(pti_generator.state_dict(), optimized_generator_path)\n",
        "\n",
        "        optimization_results.append({\n",
        "            \"original_image_path\": current_input_image_path,\n",
        "            \"original_landmarks\": original_landmarks,\n",
        "            \"optimized_latent_path\": optimized_latent_path,\n",
        "            \"optimized_generator_path\": optimized_generator_path,\n",
        "            \"output_prefix_final\": os.path.join(OUTPUT_FOLDER_PATH, current_filename_no_ext)\n",
        "        })\n",
        "        print(f\"Optimization outputs saved for {current_image_filename} to {OPTIMIZED_DATA_FOLDER}\")\n",
        "\n",
        "        del pti_model_instance, pti_generator\n",
        "        torch.cuda.empty_cache() if device == 'cuda' else None\n",
        "\n",
        "    except Exception as e_img_opt:\n",
        "        print(f\"Error optimizing {current_image_filename}: {e_img_opt}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Optimization stage (Alignment, Inversion, PTI) completed. ---\")"
      ],
      "metadata": {
        "id": "WLFD5G4UH6l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block 6 — Smile Application, Blending, and Saving\n",
        "\n",
        "Block 6 is the final execution stage where the system takes each optimized image (with its tuned generator and latent code) and applies the smile edit. It systematically loads the optimized latent and generator, injects the smile direction vector at the specified layers (typically mid-level layers controlling expression), and generates the smiling face image.\n",
        "\n",
        "Next, it warps this generated smiling face back into the coordinate space of the original high-resolution photo using a geometric transform based on the detected landmarks. A blending mask is constructed, and the smiling face is smoothly merged into the original image, preserving background details and ensuring natural integration. The block carefully manages previous outputs by deleting old files, saves both the direct smiling crop and the final blended image, and logs progress for each processed file."
      ],
      "metadata": {
        "id": "zjyfquh-PA5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMILE_ALPHA = 1.2\n",
        "SMILE_START_LAYER = 4\n",
        "SMILE_END_LAYER = 6\n",
        "\n",
        "if not optimization_results:\n",
        "    print(\"No images were successfully optimized. Skipping smile application.\")\n",
        "else:\n",
        "    print(f\"\\n--- Starting Smile Application and Blending for {len(optimization_results)} images ---\")\n",
        "\n",
        "    if 'base_e4e_opts' not in globals():\n",
        "        raise RuntimeError(\"CRITICAL: base_e4e_opts not defined. Cannot reconstruct generator for smile application.\")\n",
        "\n",
        "    for image_data in optimization_results:\n",
        "        current_input_image_path = image_data[\"original_image_path\"]\n",
        "        original_landmarks = image_data[\"original_landmarks\"]\n",
        "        optimized_latent_path = image_data[\"optimized_latent_path\"]\n",
        "        optimized_generator_path = image_data[\"optimized_generator_path\"]\n",
        "        current_output_prefix_final = image_data[\"output_prefix_final\"]\n",
        "\n",
        "        current_image_filename = os.path.basename(current_input_image_path)\n",
        "        print(f\"\\n--- Applying smile to: {current_image_filename} (Alpha: {SMILE_ALPHA}) ---\")\n",
        "\n",
        "        final_smiling_crop_path = f\"{current_output_prefix_final}_smiling_crop_alpha{SMILE_ALPHA}.jpg\"\n",
        "        final_blended_smile_path = f\"{current_output_prefix_final}_smiling_final_alpha{SMILE_ALPHA}.jpg\"\n",
        "\n",
        "        for file_to_remove in [final_smiling_crop_path, final_blended_smile_path]:\n",
        "            if os.path.exists(file_to_remove):\n",
        "                try:\n",
        "                    os.remove(file_to_remove)\n",
        "                    print(f\"Removed previous version: {file_to_remove}\")\n",
        "                except OSError as e_remove:\n",
        "                    print(f\"Error removing {file_to_remove}: {e_remove}\")\n",
        "\n",
        "        try:\n",
        "            optimized_latent = torch.load(optimized_latent_path, map_location=device)\n",
        "\n",
        "            smile_model_instance = pSp(base_e4e_opts)\n",
        "            smile_generator = smile_model_instance.decoder\n",
        "            smile_generator.load_state_dict(torch.load(optimized_generator_path, map_location=device))\n",
        "            smile_generator.to(device)\n",
        "            smile_generator.eval()\n",
        "\n",
        "            edited_latent = optimized_latent.clone().detach()\n",
        "\n",
        "            layer_to_edit = SMILE_START_LAYER\n",
        "            if layer_to_edit < edited_latent.shape[1]:\n",
        "                if smile_direction_global.ndim == 1 and smile_direction_global.shape[0] == edited_latent.shape[2]:\n",
        "                    edited_latent[:, layer_to_edit, :] += SMILE_ALPHA * smile_direction_global\n",
        "                elif smile_direction_global.ndim == 2 and smile_direction_global.shape[0] == edited_latent.shape[1] and smile_direction_global.shape[1] == edited_latent.shape[2]:\n",
        "                    edited_latent[:, layer_to_edit, :] += SMILE_ALPHA * smile_direction_global[layer_to_edit, :]\n",
        "                elif smile_direction_global.ndim == 2 and smile_direction_global.shape[0] == 1 and smile_direction_global.shape[1] == edited_latent.shape[2]:\n",
        "                    edited_latent[:, layer_to_edit, :] += SMILE_ALPHA * smile_direction_global.squeeze(0)\n",
        "                else:\n",
        "                    print(f\"Warning: Smile direction shape {smile_direction_global.shape} not directly applicable to layer {layer_to_edit}.\")\n",
        "            else:\n",
        "                print(f\"Warning: SMILE_START_LAYER {layer_to_edit} is out of bounds for latent code.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                edited_result_gen_output = smile_generator([edited_latent], input_is_latent=True, randomize_noise=False)\n",
        "                edited_image_tensor_raw = edited_result_gen_output[0] if isinstance(edited_result_gen_output, tuple) else edited_result_gen_output\n",
        "\n",
        "            final_image_norm_0_1 = (edited_image_tensor_raw.clamp(-1, 1) + 1) / 2.0\n",
        "            save_image(final_image_norm_0_1, final_smiling_crop_path)\n",
        "            print(f\"Smiling crop (alpha={SMILE_ALPHA}) saved to: {final_smiling_crop_path}\")\n",
        "\n",
        "            from scipy.ndimage import gaussian_filter\n",
        "\n",
        "            left_eye_orig_b = original_landmarks[36:42].mean(axis=0)\n",
        "            right_eye_orig_b = original_landmarks[42:48].mean(axis=0)\n",
        "            nose_orig_b = original_landmarks[30]\n",
        "            src_pts_orig_b = np.array([left_eye_orig_b, right_eye_orig_b, nose_orig_b], dtype=np.float32)\n",
        "\n",
        "            scale_1024_from_256_b = 1024 / 256\n",
        "            dst_pts_1024_b = np.array([\n",
        "                (96 * scale_1024_from_256_b, 112 * scale_1024_from_256_b),\n",
        "                (160 * scale_1024_from_256_b, 112 * scale_1024_from_256_b),\n",
        "                (128 * scale_1024_from_256_b, 160 * scale_1024_from_256_b)\n",
        "            ], dtype=np.float32)\n",
        "\n",
        "            tform_original_to_crop_b = trans.SimilarityTransform()\n",
        "            tform_original_to_crop_b.estimate(src_pts_orig_b, dst_pts_1024_b)\n",
        "\n",
        "            original_full_pil_b = Image.open(current_input_image_path).convert('RGB')\n",
        "            original_full_arr_0_1_b = np.array(original_full_pil_b) / 255.0\n",
        "\n",
        "            smiling_crop_np_0_1_b = final_image_norm_0_1.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "            warped_smiling_face_b = trans.warp(\n",
        "                smiling_crop_np_0_1_b,\n",
        "                tform_original_to_crop_b,\n",
        "                output_shape=original_full_arr_0_1_b.shape[:2],\n",
        "                mode='reflect',\n",
        "                preserve_range=True\n",
        "            )\n",
        "\n",
        "            mask_crop_space_b = np.ones((1024, 1024), dtype=np.float32)\n",
        "            warped_mask_b = trans.warp(\n",
        "                mask_crop_space_b,\n",
        "                tform_original_to_crop_b,\n",
        "                output_shape=original_full_arr_0_1_b.shape[:2],\n",
        "                preserve_range=True\n",
        "            )\n",
        "            warped_mask_b = np.clip(warped_mask_b, 0, 1)\n",
        "\n",
        "            sigma_blur_b = 0\n",
        "            blurred_mask_b = gaussian_filter(warped_mask_b, sigma=sigma_blur_b)\n",
        "            blurred_mask_b = np.clip(blurred_mask_b, 0, 1)[..., None]\n",
        "\n",
        "            blended_arr_0_1_b_final = original_full_arr_0_1_b * (1 - blurred_mask_b) + \\\n",
        "                                      warped_smiling_face_b * blurred_mask_b\n",
        "            blended_arr_0_1_b_final = np.clip(blended_arr_0_1_b_final, 0, 1)\n",
        "\n",
        "            blended_img_uint8_b_final = (blended_arr_0_1_b_final * 255).astype(np.uint8)\n",
        "            final_blended_pil_to_save = Image.fromarray(blended_img_uint8_b_final)\n",
        "            final_blended_pil_to_save.save(final_blended_smile_path)\n",
        "            print(f\"Final blended image (alpha={SMILE_ALPHA}) saved to: {final_blended_smile_path}\")\n",
        "\n",
        "            processed_image_pairs.append((current_input_image_path, final_blended_smile_path))\n",
        "\n",
        "            del smile_model_instance, smile_generator\n",
        "            torch.cuda.empty_cache() if device == 'cuda' else None\n",
        "\n",
        "        except Exception as e_smile_apply:\n",
        "            print(f\"Error applying smile to {current_image_filename}: {e_smile_apply}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Smile application and blending stage completed. ---\")\n"
      ],
      "metadata": {
        "id": "HmBsqb9bO8qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End!"
      ],
      "metadata": {
        "id": "VXNGbzwW0xCU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFjF3QmCbvPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}